{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# In this section, the main objective is to train the xgboost model and to compute the probabilities\n",
    "\n",
    "# ...............................\n",
    "# 0. Load and split the encoded data\n",
    "# ...............................\n",
    "current_dir = os.getcwd()\n",
    "path_to_csv = os.path.join(current_dir, \"data.csv\")\n",
    "df = pd.read_csv(path_to_csv, encoding='ISO-8859-1')\n",
    "\n",
    "# Split data into Train+Cal (80%) and Test (20%)\n",
    "X = df.drop(columns=['Main_mode'])\n",
    "y = df['Main_mode']\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Split Train+Cal into Train (60%) and Calibration (20%)\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.25,  # 0.25 * 80% = 20% total\n",
    "    stratify=y_tmp, random_state=42\n",
    ")\n",
    "\n",
    "# ...............................\n",
    "# 1. Log-transforming continuous valued questions to reduce the effect of the outliers\n",
    "# ...............................\n",
    "log_cols = [\n",
    "    'Work_home_distance_mode',\n",
    "    'Work_home_distance_on_foot',\n",
    "    'Work_home_travel_time_on_foot',\n",
    "    'Number_of_employees'\n",
    "]\n",
    "for col in log_cols:\n",
    "    if col in X_train:\n",
    "        X_train[col] = np.log1p(X_train[col])\n",
    "        X_cal[col]   = np.log1p(X_cal[col])\n",
    "        X_test[col]  = np.log1p(X_test[col])\n",
    "\n",
    "# ...............................\n",
    "# 2. Identify numerical columns to normalize\n",
    "# ...............................\n",
    "# Load metadata\n",
    "path_to_json = os.path.join(current_dir, \"question_types.json\")\n",
    "with open(path_to_json, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "numerical_cols = [\n",
    "    col for col, meta in metadata.items()\n",
    "    if meta.get('type') == 'numerical_value' and col in X_train.columns\n",
    "]\n",
    "# ...............................\n",
    "# 3. Build preprocessing + model pipeline\n",
    "# ...............................\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ...............................\n",
    "# 4. Hyperparameter tuning with CV on the Train set\n",
    "# ...............................\n",
    "param_grid = {\n",
    "    'clf__max_depth': [4, 6, 8],\n",
    "    'clf__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'clf__reg_lambda': [0, 0.1, 1, 5]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Grid search took {time.time() - start_time:.1f}s\")\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# ...............................\n",
    "# 5. Final evaluation on Test set\n",
    "# ...............................\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# In this section, we build up the conformal prediction layer using the calibration set and then construct\n",
    "# the prediction sets using the test set. We then calculate the average set size and the global coverage\n",
    "\n",
    "# ...............................\n",
    "# 6. Inductive Mondrian Conformal Prediction (Building the layer and collecting the non-conformity scores)\n",
    "# ...............................\n",
    "P_cal = best_pipeline.predict_proba(X_cal)\n",
    "classes = best_pipeline.classes_\n",
    "alpha_mis = 0.1 # Which corresponds to 90 % of coverage\n",
    "\n",
    "# 6a. Compute non-conformity scores per class\n",
    "scores_by_class = {ℓ: [] for ℓ in classes}\n",
    "for i, ℓ_true in enumerate(y_cal):\n",
    "    p_true = P_cal[i, np.where(classes == ℓ_true)[0][0]]\n",
    "    scores_by_class[ℓ_true].append(1.0 - p_true)\n",
    "\n",
    "# 6b. Compute τ_ℓ using classic conformal quantile rule\n",
    "tau_by_class = {}\n",
    "for ℓ, scores in scores_by_class.items():\n",
    "    scores = np.asarray(scores)\n",
    "    n = len(scores)\n",
    "    q_level = np.ceil((n + 1) * (1 - alpha_mis)) / n\n",
    "    q_level = min(q_level, 1.0)\n",
    "    tau_by_class[ℓ] = np.quantile(scores, q_level, method=\"higher\")\n",
    "\n",
    "print(\"Class-conditional τ thresholds (linear interpolation):\", tau_by_class)\n",
    "\n",
    "# ...............................\n",
    "# 7. Mondrian CP prediction sets on Test set (Using the test sets to get the prediction sets)\n",
    "# -------------------------------\n",
    "P_test = best_pipeline.predict_proba(X_test)\n",
    "prediction_sets = []\n",
    "for i in range(len(X_test)):\n",
    "    current_set = []\n",
    "    for j, ℓ in enumerate(classes):\n",
    "        nonconf_score = 1.0 - P_test[i, j]\n",
    "        if nonconf_score <= tau_by_class[ℓ]:\n",
    "            current_set.append(ℓ)\n",
    "    prediction_sets.append(current_set)\n",
    "\n",
    "# 7a. Evaluate coverage and set size\n",
    "coverage = np.mean([y_test.iloc[i] in s for i, s in enumerate(prediction_sets)])\n",
    "avg_set_size = np.mean([len(s) for s in prediction_sets])\n",
    "print(f\"Mondrian CP Coverage: {coverage:.3f}, Avg set size: {avg_set_size:.2f}\")"
   ],
   "id": "5eade3a0fb9f7fcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Having created the prediction sets, we then obtain the class conditional coverages\n",
    "\n",
    "# ...............................\n",
    "# 8. Get class conditional coverages\n",
    "# ...............................\n",
    "# 8a. Getting the classes from the fitted pipeline\n",
    "classes = best_pipeline.classes_\n",
    "\n",
    "# 8b. map numeric codes → mode names\n",
    "label_map = {\n",
    "    0: \"feet\",\n",
    "    1: \"auto\",\n",
    "    2: \"bike\",\n",
    "    3: \"PT\",\n",
    "    4: \"motorbike\",\n",
    "    5: \"multimodal\"\n",
    "}\n",
    "\n",
    "# 8c. Compute coverage per class\n",
    "coverage_by_class = {}\n",
    "for ℓ in classes:\n",
    "    idxs = np.where(y_test.to_numpy() == ℓ)[0]\n",
    "    cov = np.mean([ℓ in prediction_sets[i] for i in idxs])\n",
    "    coverage_by_class[ℓ] = cov\n",
    "\n",
    "# 8d. Display the results\n",
    "df_cov = pd.DataFrame.from_dict(coverage_by_class, orient='index', columns=['coverage'])\n",
    "df_cov.index.name = 'class_code'\n",
    "df_cov.reset_index(inplace=True)\n",
    "df_cov['mode_name'] = df_cov['class_code'].map(label_map)\n",
    "\n",
    "print(df_cov[['mode_name','coverage']])"
   ],
   "id": "1d981f4d54103c80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Given the effect of finite sample variations, we further enhance our conformal prediction layer using\n",
    "# the wilson intervals\n",
    "\n",
    "# ...............................\n",
    "# 9. Compute Wilson interval for each class\n",
    "# ...............................\n",
    "\n",
    "# 9a. Compute the intervals\n",
    "z = 1.96  # This is the standard-normal interval half-width value corresponding to 95% confidence interval\n",
    "intervals = []\n",
    "\n",
    "for ℓ in classes:\n",
    "    idxs = np.where(y_test.to_numpy() == ℓ)[0]\n",
    "    n = len(idxs)\n",
    "    p_hat = coverage_by_class[ℓ]\n",
    "\n",
    "    denominator = 1 + (z**2) / n\n",
    "    center = (p_hat + (z**2) / (2 * n)) / denominator\n",
    "    margin = (z * np.sqrt((p_hat * (1 - p_hat) / n) + (z**2 / (4 * n**2)))) / denominator\n",
    "\n",
    "    lower = max(0.0, center - margin)\n",
    "    upper = min(1.0, center + margin)\n",
    "\n",
    "    intervals.append((ℓ, label_map[ℓ], p_hat, lower, upper))\n",
    "\n",
    "# 9b. Display the results\n",
    "df_intervals = pd.DataFrame(intervals, columns=['class_code', 'mode_name', 'coverage', 'wilson_lower', 'wilson_upper'])\n",
    "print(df_intervals)"
   ],
   "id": "8162c891ed4ccb86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
